{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f41742",
   "metadata": {},
   "source": [
    "# Example training job using pytorch lightning with local gpu compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a71662",
   "metadata": {},
   "source": [
    "This notebook contains steps to download data and run a training job using pytorch lightning with distributed training on multi-core gpu instances. Note that this code is tested on a SageMaker notebook instance with ml.p3.16xlarge (8gpu cores). The kernel used is `conda_pytorch_p38`\n",
    "\n",
    "Steps in the notebook:\n",
    "1. download and prepare the KITTI Semantic Segmentation Benchmark dataset.\n",
    "2. install addtional packages\n",
    "3. run training job in a python script\n",
    "\n",
    "The training code is developed based on the original source on [Lighting-AI](https://github.com/Lightning-AI/lightning/blob/9cc714cdd12b90faea1b4fc7265dd384b224792e/src/pytorch_lightning/trainer/trainer.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ab027",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_semantics.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed0bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data_semantics.zip -d data_semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8a1de",
   "metadata": {},
   "source": [
    "### install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae6f2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r code/requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r code/requirements.txt (line 2)) (0.11.1)\n",
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-1.7.1-py3-none-any.whl (701 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.5/701.5 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torch->-r code/requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision->-r code/requirements.txt (line 2)) (1.21.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision->-r code/requirements.txt (line 2)) (9.0.1)\n",
      "Collecting pyDeprecate>=0.3.1\n",
      "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning->-r code/requirements.txt (line 3)) (4.62.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning->-r code/requirements.txt (line 3)) (5.4.1)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.6/419.6 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning->-r code/requirements.txt (line 3)) (21.3)\n",
      "Collecting tensorboard>=2.9.1\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning->-r code/requirements.txt (line 3)) (2021.11.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (2.26.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning->-r code/requirements.txt (line 3)) (3.0.6)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.10.0-py2.py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.2/167.2 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning->-r code/requirements.txt (line 3)) (3.19.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning->-r code/requirements.txt (line 3)) (59.2.0)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning->-r code/requirements.txt (line 3)) (0.37.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning->-r code/requirements.txt (line 3)) (2.0.3)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 KB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->-r code/requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->-r code/requirements.txt (line 3)) (4.7.2)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning->-r code/requirements.txt (line 3)) (4.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (2021.10.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (5.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r code/requirements.txt (line 3)) (1.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning->-r code/requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->-r code/requirements.txt (line 3)) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 KB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, tensorboard-data-server, pyDeprecate, pyasn1-modules, oauthlib, grpcio, cachetools, absl-py, torchmetrics, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, pytorch-lightning\n",
      "Successfully installed absl-py-1.2.0 cachetools-5.2.0 google-auth-2.10.0 google-auth-oauthlib-0.4.6 grpcio-1.47.0 markdown-3.4.1 oauthlib-3.2.0 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pytorch-lightning-1.7.1 requests-oauthlib-1.3.1 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.9.3\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade future\n",
    "! pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c33f92",
   "metadata": {},
   "source": [
    "### run python script\n",
    "This script will use the data stored locally to train a sematic segmentation model and save the trained model as a '.pth' file at the local directory.\n",
    "Note that in this example, we set the precision to 16-bit floating-point which requires pytorch version equal to or greater than 1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17d47bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | net  | UNet | 31.0 M\n",
      "------------------------------\n",
      "31.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 M    Total params\n",
      "62.089    Total estimated model params size (MB)\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Sanity Checking: 0it [00:00, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:00<00:00,  4.54it/s]/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1894: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:   0%|                                            | 0/7 [00:00<?, ?it/s][W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  71%|███████████▍    | 5/7 [00:07<00:03,  1.53s/it, loss=2.45, v_num=5]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|█████████████▋  | 6/7 [00:09<00:01,  1.51s/it, loss=2.45, v_num=5]\u001b[A\n",
      "Epoch 0: 100%|████████████████| 7/7 [00:09<00:00,  1.33s/it, loss=2.45, v_num=5]\u001b[A\n",
      "Epoch 1:  71%|███████████▍    | 5/7 [00:06<00:02,  1.20s/it, loss=2.19, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.20s/it, loss=2.19, v_num=5]\u001b[A\n",
      "Epoch 1: 100%|████████████████| 7/7 [00:07<00:00,  1.07s/it, loss=2.19, v_num=5]\u001b[A\n",
      "Epoch 2:  71%|███████████▍    | 5/7 [00:06<00:02,  1.30s/it, loss=2.02, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.26s/it, loss=2.02, v_num=5]\u001b[A\n",
      "Epoch 2: 100%|████████████████| 7/7 [00:07<00:00,  1.12s/it, loss=2.02, v_num=5]\u001b[A\n",
      "Epoch 3:  71%|███████████▍    | 5/7 [00:06<00:02,  1.33s/it, loss=1.87, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.32s/it, loss=1.87, v_num=5]\u001b[A\n",
      "Epoch 3: 100%|████████████████| 7/7 [00:08<00:00,  1.18s/it, loss=1.87, v_num=5]\u001b[A\n",
      "Epoch 4:  71%|███████████▍    | 5/7 [00:06<00:02,  1.25s/it, loss=1.61, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.25s/it, loss=1.61, v_num=5]\u001b[A\n",
      "Epoch 4: 100%|████████████████| 7/7 [00:07<00:00,  1.11s/it, loss=1.61, v_num=5]\u001b[A\n",
      "Epoch 4: 100%|████████████████| 7/7 [00:07<00:00,  1.12s/it, loss=1.61, v_num=5]\u001b[A`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "Epoch 4: 100%|████████████████| 7/7 [00:09<00:00,  1.33s/it, loss=1.61, v_num=5]\n"
     ]
    }
   ],
   "source": [
    "!python code/semantic_segmentation_single.py --data_path data_semantics --batch_size=4 --max_epochs 5 --model_dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70fc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede56d40",
   "metadata": {},
   "source": [
    "While the code is running, you can open a Terminal session and run \"\\$nvidia-smi\" to check the gpu utilisation.\n",
    "You can see the output as below:\n",
    "![nvidia-smi](img/nvidia-smi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a3060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
