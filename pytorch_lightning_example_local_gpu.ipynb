{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ce4299",
   "metadata": {},
   "source": [
    "# Example training job using pytorch lightning with local gpu compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39d0b4",
   "metadata": {},
   "source": [
    "This notebook contains steps to download data and run a training job using pytorch lightning with distributed training on multi-core gpu instances. Note that this code is tested on a SageMaker notebook instance with ml.p3.16xlarge (8gpu cores). The kernel used is `conda_pytorch_p38`\n",
    "\n",
    "Steps in the notebook:\n",
    "1. download and prepare the KITTI Semantic Segmentation Benchmark dataset.\n",
    "2. install addtional packages\n",
    "3. run training job in a python script\n",
    "\n",
    "The training code is developed based on the original source on [Lighting-AI](https://github.com/Lightning-AI/lightning/blob/9cc714cdd12b90faea1b4fc7265dd384b224792e/src/pytorch_lightning/trainer/trainer.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6148b6b",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_semantics.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ca56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data_semantics.zip -d data_semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f73e28",
   "metadata": {},
   "source": [
    "### install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade future\n",
    "! pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99485c1f",
   "metadata": {},
   "source": [
    "### run python script\n",
    "This script will use the data stored locally to train a sematic segmentation model and save the trained model as a '.pth' file at the local directory.\n",
    "Note that in this example, we set the precision to 16-bit floating-point which requires pytorch version equal to or greater than 1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf9bc06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | net  | UNet | 31.0 M\n",
      "------------------------------\n",
      "31.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 M    Total params\n",
      "62.089    Total estimated model params size (MB)\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Sanity Checking: 0it [00:00, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:00<00:00,  4.54it/s]/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1894: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:   0%|                                            | 0/7 [00:00<?, ?it/s][W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  71%|███████████▍    | 5/7 [00:07<00:03,  1.53s/it, loss=2.45, v_num=5]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|█████████████▋  | 6/7 [00:09<00:01,  1.51s/it, loss=2.45, v_num=5]\u001b[A\n",
      "Epoch 0: 100%|████████████████| 7/7 [00:09<00:00,  1.33s/it, loss=2.45, v_num=5]\u001b[A\n",
      "Epoch 1:  71%|███████████▍    | 5/7 [00:06<00:02,  1.20s/it, loss=2.19, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.20s/it, loss=2.19, v_num=5]\u001b[A\n",
      "Epoch 1: 100%|████████████████| 7/7 [00:07<00:00,  1.07s/it, loss=2.19, v_num=5]\u001b[A\n",
      "Epoch 2:  71%|███████████▍    | 5/7 [00:06<00:02,  1.30s/it, loss=2.02, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.26s/it, loss=2.02, v_num=5]\u001b[A\n",
      "Epoch 2: 100%|████████████████| 7/7 [00:07<00:00,  1.12s/it, loss=2.02, v_num=5]\u001b[A\n",
      "Epoch 3:  71%|███████████▍    | 5/7 [00:06<00:02,  1.33s/it, loss=1.87, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.32s/it, loss=1.87, v_num=5]\u001b[A\n",
      "Epoch 3: 100%|████████████████| 7/7 [00:08<00:00,  1.18s/it, loss=1.87, v_num=5]\u001b[A\n",
      "Epoch 4:  71%|███████████▍    | 5/7 [00:06<00:02,  1.25s/it, loss=1.61, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.25s/it, loss=1.61, v_num=5]\u001b[A\n",
      "Epoch 4: 100%|████████████████| 7/7 [00:07<00:00,  1.11s/it, loss=1.61, v_num=5]\u001b[A\n",
      "Epoch 4: 100%|████████████████| 7/7 [00:07<00:00,  1.12s/it, loss=1.61, v_num=5]\u001b[A`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "Epoch 4: 100%|████████████████| 7/7 [00:09<00:00,  1.33s/it, loss=1.61, v_num=5]\n"
     ]
    }
   ],
   "source": [
    "!python code/semantic_segmentation.py --data_path data_semantics --batch_size=4 --max_epochs 5 --model_dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab81ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43f3e805",
   "metadata": {},
   "source": [
    "While the code is running, you can open a Terminal session and run \"\\$nvidia-smi\" to check the gpu utilisation.\n",
    "You can see the output as below:\n",
    "![nvidia-smi](img/nvidia-smi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1d1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
