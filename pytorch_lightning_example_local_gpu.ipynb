{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a706a5b1",
   "metadata": {},
   "source": [
    "# Example training job using pytorch lightning with local gpu compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1ad9f",
   "metadata": {},
   "source": [
    "This notebook contains steps to download data and run a training job using pytorch lightning with distributed training on multi-core gpu instances. Note that this code is tested on a SageMaker notebook instance with ml.p3.16xlarge (8gpu cores). The kernel used is `conda_pytorch_p38`\n",
    "\n",
    "Steps in the notebook:\n",
    "1. download and prepare the KITTI Semantic Segmentation Benchmark dataset.\n",
    "2. install addtional packages\n",
    "3. run training job in a python script\n",
    "\n",
    "The training code is developed based on the original source on [Lighting-AI](https://github.com/Lightning-AI/lightning/blob/9cc714cdd12b90faea1b4fc7265dd384b224792e/src/pytorch_lightning/trainer/trainer.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ffcbf",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9445c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_semantics.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data_semantics.zip -d data_semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b442d",
   "metadata": {},
   "source": [
    "### install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabccbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade future\n",
    "! pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fb6e8",
   "metadata": {},
   "source": [
    "### run python script\n",
    "This script will use the data stored locally to train a sematic segmentation model and save the trained model as a '.pth' file at the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fea345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_semantics\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "data_semantics\n",
      "Input file list: []\n",
      "data_semantics\n",
      "Input file list: []\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | net  | UNet | 31.0 M\n",
      "------------------------------\n",
      "31.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 M    Total params\n",
      "62.089    Total estimated model params size (MB)\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Sanity Checking: 0it [00:00, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:00<00:00,  3.54it/s]/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1894: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:   0%|                                            | 0/7 [00:00<?, ?it/s][W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  71%|████████████▏    | 5/7 [00:07<00:03,  1.51s/it, loss=2.5, v_num=2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|██████████████▌  | 6/7 [00:08<00:01,  1.45s/it, loss=2.5, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|█████████████████| 7/7 [00:09<00:00,  1.29s/it, loss=2.5, v_num=2]\u001b[A\n",
      "Epoch 1:  71%|███████████▍    | 5/7 [00:06<00:02,  1.38s/it, loss=2.28, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.32s/it, loss=2.28, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████████████| 7/7 [00:08<00:00,  1.16s/it, loss=2.28, v_num=2]\u001b[A\n",
      "Epoch 2:  71%|███████████▍    | 5/7 [00:06<00:02,  1.35s/it, loss=2.12, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.32s/it, loss=2.12, v_num=2]\u001b[A\n",
      "Epoch 2: 100%|████████████████| 7/7 [00:08<00:00,  1.18s/it, loss=2.12, v_num=2]\u001b[A\n",
      "Epoch 3:  71%|███████████▍    | 5/7 [00:07<00:02,  1.42s/it, loss=2.01, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  86%|█████████████▋  | 6/7 [00:08<00:01,  1.34s/it, loss=2.01, v_num=2]\u001b[A\n",
      "Epoch 3: 100%|████████████████| 7/7 [00:08<00:00,  1.18s/it, loss=2.01, v_num=2]\u001b[A\n",
      "Epoch 4:  71%|███████████▍    | 5/7 [00:07<00:02,  1.48s/it, loss=1.74, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  86%|█████████████▋  | 6/7 [00:08<00:01,  1.43s/it, loss=1.74, v_num=2]\u001b[A\n",
      "Epoch 4: 100%|████████████████| 7/7 [00:09<00:00,  1.29s/it, loss=1.74, v_num=2]\u001b[A\n",
      "Epoch 5:  71%|███████████▍    | 5/7 [00:07<00:02,  1.47s/it, loss=1.62, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  86%|█████████████▋  | 6/7 [00:08<00:01,  1.44s/it, loss=1.62, v_num=2]\u001b[A\n",
      "Epoch 5: 100%|████████████████| 7/7 [00:08<00:00,  1.28s/it, loss=1.62, v_num=2]\u001b[A\n",
      "Epoch 6:  71%|████████████▏    | 5/7 [00:06<00:02,  1.34s/it, loss=1.5, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  86%|██████████████▌  | 6/7 [00:07<00:01,  1.33s/it, loss=1.5, v_num=2]\u001b[A\n",
      "Epoch 6: 100%|█████████████████| 7/7 [00:08<00:00,  1.18s/it, loss=1.5, v_num=2]\u001b[A\n",
      "Epoch 7:  71%|███████████▍    | 5/7 [00:06<00:02,  1.39s/it, loss=1.45, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  86%|█████████████▋  | 6/7 [00:08<00:01,  1.38s/it, loss=1.45, v_num=2]\u001b[A\n",
      "Epoch 7: 100%|████████████████| 7/7 [00:08<00:00,  1.22s/it, loss=1.45, v_num=2]\u001b[A\n",
      "Epoch 8:  71%|████████████▏    | 5/7 [00:06<00:02,  1.35s/it, loss=1.4, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  86%|██████████████▌  | 6/7 [00:07<00:01,  1.29s/it, loss=1.4, v_num=2]\u001b[A\n",
      "Epoch 8: 100%|█████████████████| 7/7 [00:08<00:00,  1.16s/it, loss=1.4, v_num=2]\u001b[A\n",
      "Epoch 9:  71%|███████████▍    | 5/7 [00:06<00:02,  1.38s/it, loss=1.31, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  86%|█████████████▋  | 6/7 [00:07<00:01,  1.27s/it, loss=1.31, v_num=2]\u001b[A\n",
      "Epoch 9: 100%|████████████████| 7/7 [00:07<00:00,  1.12s/it, loss=1.31, v_num=2]\u001b[A\n",
      "Epoch 10:  71%|███████████▍    | 5/7 [00:07<00:02,  1.42s/it, loss=1.3, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  86%|█████████████▋  | 6/7 [00:08<00:01,  1.38s/it, loss=1.3, v_num=2]\u001b[A\n",
      "Epoch 10: 100%|████████████████| 7/7 [00:08<00:00,  1.20s/it, loss=1.3, v_num=2]\u001b[A\n",
      "Epoch 11:  71%|██████████▋    | 5/7 [00:06<00:02,  1.31s/it, loss=1.26, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  86%|████████████▊  | 6/7 [00:07<00:01,  1.31s/it, loss=1.26, v_num=2]\u001b[A\n",
      "Epoch 11: 100%|███████████████| 7/7 [00:08<00:00,  1.18s/it, loss=1.26, v_num=2]\u001b[A\n",
      "Epoch 12:  71%|██████████▋    | 5/7 [00:06<00:02,  1.26s/it, loss=1.26, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  86%|████████████▊  | 6/7 [00:07<00:01,  1.24s/it, loss=1.26, v_num=2]\u001b[A\n",
      "Epoch 12: 100%|███████████████| 7/7 [00:07<00:00,  1.10s/it, loss=1.26, v_num=2]\u001b[A\n",
      "Epoch 13:  71%|██████████▋    | 5/7 [00:06<00:02,  1.38s/it, loss=1.24, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  86%|████████████▊  | 6/7 [00:07<00:01,  1.33s/it, loss=1.24, v_num=2]\u001b[A\n",
      "Epoch 13: 100%|███████████████| 7/7 [00:08<00:00,  1.16s/it, loss=1.24, v_num=2]\u001b[A\n",
      "Epoch 14:  71%|██████████▋    | 5/7 [00:06<00:02,  1.36s/it, loss=1.24, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  86%|████████████▊  | 6/7 [00:08<00:01,  1.33s/it, loss=1.24, v_num=2]\u001b[A\n",
      "Epoch 14: 100%|███████████████| 7/7 [00:08<00:00,  1.16s/it, loss=1.24, v_num=2]\u001b[A\n",
      "Epoch 15:  71%|██████████▋    | 5/7 [00:07<00:02,  1.45s/it, loss=1.21, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  86%|████████████▊  | 6/7 [00:08<00:01,  1.38s/it, loss=1.21, v_num=2]\u001b[A\n",
      "Epoch 15: 100%|███████████████| 7/7 [00:08<00:00,  1.22s/it, loss=1.21, v_num=2]\u001b[A\n",
      "Epoch 16:  71%|██████████▋    | 5/7 [00:07<00:02,  1.42s/it, loss=1.21, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  86%|████████████▊  | 6/7 [00:07<00:01,  1.33s/it, loss=1.21, v_num=2]\u001b[A\n",
      "Epoch 16: 100%|███████████████| 7/7 [00:08<00:00,  1.18s/it, loss=1.21, v_num=2]\u001b[A\n",
      "Epoch 17:  71%|██████████▋    | 5/7 [00:06<00:02,  1.31s/it, loss=1.21, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  86%|████████████▊  | 6/7 [00:07<00:01,  1.29s/it, loss=1.21, v_num=2]\u001b[A\n",
      "Epoch 17: 100%|███████████████| 7/7 [00:07<00:00,  1.13s/it, loss=1.21, v_num=2]\u001b[A\n",
      "Epoch 18:  71%|███████████▍    | 5/7 [00:07<00:02,  1.41s/it, loss=1.2, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  86%|█████████████▋  | 6/7 [00:08<00:01,  1.34s/it, loss=1.2, v_num=2]\u001b[A\n",
      "Epoch 18: 100%|████████████████| 7/7 [00:08<00:00,  1.19s/it, loss=1.2, v_num=2]\u001b[A\n",
      "Epoch 19:  71%|███████████▍    | 5/7 [00:06<00:02,  1.37s/it, loss=1.2, v_num=2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  86%|█████████████▋  | 6/7 [00:08<00:01,  1.34s/it, loss=1.2, v_num=2]\u001b[A\n",
      "Epoch 19: 100%|████████████████| 7/7 [00:08<00:00,  1.18s/it, loss=1.2, v_num=2]\u001b[A\n",
      "Epoch 19: 100%|████████████████| 7/7 [00:08<00:00,  1.24s/it, loss=1.2, v_num=2]\u001b[A`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "Epoch 19: 100%|████████████████| 7/7 [00:10<00:00,  1.47s/it, loss=1.2, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "!python code/semantic_segmentation.py --data_path data_semantics --batch_size=4 --model_dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93c368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
