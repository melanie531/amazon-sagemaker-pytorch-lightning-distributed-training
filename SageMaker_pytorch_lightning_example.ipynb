{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282da29e",
   "metadata": {},
   "source": [
    "# Train Sementic Segmentation model using Pytorch Lightning on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528acf5",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook will demonstrate how you can train a semantic segmentation model by using custom training script with Pytorch lightning, similar to those you would use outside of SageMaker, with SageMaker's prebuilt containers for various frameworks.\n",
    "\n",
    "SageMaker Script Mode is flexible so you'll also be seeing examples of how to include your own dependencies, such as a custom Python library, in your training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45adeb",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "To follow along, you need to create an IAM role, SageMaker Notebook instance, and S3 bucket. \n",
    "Once the SageMaker Notebook instance is created, choose conda_python3 as the kernel.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77f022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f76eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (2.103.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (21.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.3.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.21.2)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (4.8.2)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (1.24.42)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker) (3.19.4)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.42 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.51)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (3.0.6)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.42->boto3<2.0,>=1.20.21->sagemaker) (1.26.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "original_version = sagemaker.__version__\n",
    "if sagemaker.__version__ != \"2.103.1\":\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"sagemaker\"])\n",
    "    import importlib\n",
    "\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d05bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this step if you have already downloaded and unzipped the data\n",
    "!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_semantics.zip\n",
    "!unzip data_semantics.zip -d data_semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a747846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redirecting to /bin/systemctl stop docker.service\n",
      "Warning: Stopping docker.service, but it can still be activated by:\n",
      "  docker.socket\n",
      "Redirecting to /bin/systemctl start docker.service\n"
     ]
    }
   ],
   "source": [
    "# only run the below cells when you are using sagemaker notebook instances\n",
    "!bash ./prepare-docker.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e7fa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-docker2 already installed. We are good to go!\n",
      "SageMaker instance route table setup is ok. We are good to go.\n",
      "SageMaker instance routing for Docker is ok. We are good to go!\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/local_mode_setup.sh\n",
    "!/bin/bash ./local_mode_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c442d9",
   "metadata": {},
   "source": [
    "### define parameters and upload files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca3d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session = sagemaker.session.Session()\n",
    "bucket = sm_session.default_bucket()\n",
    "prefix = 'sagemaker/pytorch-lightning-example'\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9404a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to s3://sagemaker-us-east-1-631450739534/sagemaker/pytorch-lightning-example/data\n"
     ]
    }
   ],
   "source": [
    "data_path = s3_path_join(\"s3://\", bucket, prefix + \"/data\")\n",
    "print(f\"Uploading data to {data_path}\")\n",
    "data_url = S3Uploader.upload('data_semantics', data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c784f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-631450739534/sagemaker/pytorch-lightning-example/data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_url #= 's3://sagemaker-us-east-1-631450739534/sagemaker/pytorch-lightning-example/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627d8bf",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "In this PyTorch example, we show how to using pytorch lightning to train a semantic segmentation model with multiple gpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5916c86a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 5jfl2hnhwa-algo-1-5y4z8 ... \n",
      "Creating 5jfl2hnhwa-algo-1-5y4z8 ... done\n",
      "Attaching to 5jfl2hnhwa-algo-1-5y4z8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:20,901 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:20,977 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:20,978 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:20,981 sagemaker_pytorch_container.training INFO     Pytorch_ddp_enabled is:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:20,981 sagemaker_pytorch_container.training INFO     True\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:20,982 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:20,982 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:21,209 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m /opt/conda/bin/python3.8 -m pip install -r requirements.txt\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.12.0+cu113)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.13.0+cu113)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.6.3)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (2.28.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (1.22.2)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (9.2.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.3.2)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: torchmetrics>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.9.3)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (5.4.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2022.5.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (21.3)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (4.64.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning->-r requirements.txt (line 3)) (3.0.9)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.47.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.6)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.4.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (63.2.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.1.2)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.37.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.6.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.19.4)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.10)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.1.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.6.15)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.16.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (5.2.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.7.2)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.2.8)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.12.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (4.0.2)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (21.4.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (6.0.2)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.8)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.2.0)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [notice] A new release of pip available: 22.2 -> 22.2.2\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [notice] To update, run: pip install --upgrade pip\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,143 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,143 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,225 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,304 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,305 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,305 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,309 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,310 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,310 sagemaker-training-toolkit INFO     Host: ['algo-1-5y4z8']\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,311 sagemaker-training-toolkit INFO     instance type: local_gpu\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,311 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1-5y4z8'] Hosts: ['algo-1-5y4z8'] process_per_hosts: 8 num_processes: 8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,386 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:49:23,386 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m \n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Training Env:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m \n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m {\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"additional_framework_parameters\": {\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"sagemaker_pytorch_ddp_enabled\": true,\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"sagemaker_instance_type\": \"local_gpu\"\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     },\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"data_path\": \"/opt/ml/input/data/data_path\"\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     },\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"current_host\": \"algo-1-5y4z8\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"current_instance_group\": \"homogeneousCluster\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"current_instance_group_hosts\": [],\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"current_instance_type\": \"local\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"distribution_hosts\": [\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"algo-1-5y4z8\"\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     ],\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"distribution_instance_groups\": [\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"homogeneousCluster\"\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     ],\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"algo-1-5y4z8\"\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     ],\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"batch_size\": 8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     },\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"data_path\": {\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         }\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     },\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"instance_groups\": [],\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"instance_groups_dict\": {},\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"is_hetero\": false,\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"is_modelparallel_enabled\": null,\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"job_name\": \"pytorch-lightning-2022-08-16-02-49-17-400\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"master_hostname\": \"algo-1-5y4z8\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-631450739534/pytorch-lightning-2022-08-16-02-49-17-400/source/sourcedir.tar.gz\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"module_name\": \"semantic_segmentation_single\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"current_host\": \"algo-1-5y4z8\",\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m             \"algo-1-5y4z8\"\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m         ]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     },\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m     \"user_entry_point\": \"semantic_segmentation_single.py\"\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m }\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m \n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Environment variables:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m \n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_HOSTS=[\"algo-1-5y4z8\"]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_HPS={\"batch_size\":8}\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_USER_ENTRY_POINT=semantic_segmentation_single.py\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"local_gpu\",\"sagemaker_pytorch_ddp_enabled\":true}\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-5y4z8\",\"hosts\":[\"algo-1-5y4z8\"]}\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_INPUT_DATA_CONFIG={\"data_path\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_CHANNELS=[\"data_path\"]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_CURRENT_HOST=algo-1-5y4z8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_CURRENT_INSTANCE_TYPE=local\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_INSTANCE_GROUPS=[]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_INSTANCE_GROUPS_DICT={}\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_IS_HETERO=false\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_MODULE_NAME=semantic_segmentation_single\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-631450739534/pytorch-lightning-2022-08-16-02-49-17-400/source/sourcedir.tar.gz\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"local_gpu\",\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{\"data_path\":\"/opt/ml/input/data/data_path\"},\"current_host\":\"algo-1-5y4z8\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-5y4z8\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-5y4z8\"],\"hyperparameters\":{\"batch_size\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"data_path\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-lightning-2022-08-16-02-49-17-400\",\"log_level\":20,\"master_hostname\":\"algo-1-5y4z8\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-631450739534/pytorch-lightning-2022-08-16-02-49-17-400/source/sourcedir.tar.gz\",\"module_name\":\"semantic_segmentation_single\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-5y4z8\",\"hosts\":[\"algo-1-5y4z8\"]},\"user_entry_point\":\"semantic_segmentation_single.py\"}\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"8\"]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_CHANNEL_DATA_PATH=/opt/ml/input/data/data_path\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m SM_HP_BATCH_SIZE=8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m \n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m \n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m mpirun --host algo-1-5y4z8 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so smddprun /opt/conda/bin/python3.8 -m mpi4py semantic_segmentation_single.py --batch_size 8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m \n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m \n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Using 16bit native Automatic Mixed Precision (AMP)\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:GPU available: True, used: True\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:IPU available: False, using: 0 IPUs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:HPU available: False, using: 0 HPUs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:----------------------------------------------------------------------------------------------------\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:distributed_backend=nccl\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:All distributed processes registered. Starting with 8 processes\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:----------------------------------------------------------------------------------------------------\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] NCCL INFO Bootstrap : Using eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] NCCL INFO Using network Socket\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:134 [7] NCCL INFO Bootstrap : Using eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:134 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:134 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:134 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:131 [5] NCCL INFO Bootstrap : Using eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:131 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:131 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:131 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:134 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:134 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:134 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:134 [7] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:134 [7] NCCL INFO Using network Socket\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:131 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:131 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:131 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:133 [6] NCCL INFO Bootstrap : Using eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:131 [5] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:131 [5] NCCL INFO Using network Socket\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:133 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:133 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:133 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:124 [2] NCCL INFO Bootstrap : Using eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:129 [4] NCCL INFO Bootstrap : Using eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:121 [1] NCCL INFO Bootstrap : Using eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:124 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:124 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:124 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:129 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:129 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:129 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:121 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:121 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:121 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:133 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:133 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:133 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:133 [6] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:133 [6] NCCL INFO Using network Socket\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:124 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:124 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:124 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:124 [2] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:124 [2] NCCL INFO Using network Socket\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:129 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:129 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:129 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:121 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:121 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:129 [4] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:129 [4] NCCL INFO Using network Socket\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:121 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:121 [1] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:121 [1] NCCL INFO Using network Socket\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:127 [3] NCCL INFO Bootstrap : Using eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:127 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:127 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:127 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:127 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:127 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:127 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:127 [3] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:127 [3] NCCL INFO Using network Socket\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 2/-1/-1->1->5 [3] 2/-1/-1->1->5 [4] 3/-1/-1->1->0 [5] -1/-1/-1->1->3 [6] 5/-1/-1->1->2 [7] 5/-1/-1->1->2 [8] 2/-1/-1->1->5 [9] 2/-1/-1->1->5 [10] 3/-1/-1->1->0 [11] -1/-1/-1->1->3\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] -1/-1/-1->2->6 [5] 6/-1/-1->2->0 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->6 [11] 6/-1/-1->2->0\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] 7/-1/-1->4->0 [3] 7/-1/-1->4->0 [4] 6/-1/-1->4->5 [5] 5/-1/-1->4->6 [6] -1/-1/-1->4->7 [7] -1/-1/-1->4->7 [8] 7/-1/-1->4->0 [9] 7/-1/-1->4->0 [10] 6/-1/-1->4->5 [11] 5/-1/-1->4->6\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 7/-1/-1->3->1 [5] 1/-1/-1->3->7 [6] 2/-1/-1->3->0 [7] 2/-1/-1->3->0 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 7/-1/-1->3->1 [11] 1/-1/-1->3->7\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 2/-1/-1->6->4 [5] 4/-1/-1->6->2 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 2/-1/-1->6->4 [11] 4/-1/-1->6->2\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 1/-1/-1->5->6 [3] 1/-1/-1->5->6 [4] 4/-1/-1->5->7 [5] 7/-1/-1->5->4 [6] 6/-1/-1->5->1 [7] 6/-1/-1->5->1 [8] 1/-1/-1->5->6 [9] 1/-1/-1->5->6 [10] 4/-1/-1->5->7 [11] 7/-1/-1->5->4\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 4/-1/-1->0->-1 [3] 4/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 4/-1/-1->0->-1 [9] 4/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Connected all rings\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Connected all rings\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Connected all rings\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Connected all rings\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Connected all rings\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Connected all rings\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Connected all rings\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Connected all rings\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Connected all trees\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Connected all trees\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Connected all trees\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Connected all trees\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Connected all trees\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Connected all trees\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Connected all trees\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Connected all trees\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 01 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 01 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 09 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 02 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 02 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 09 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 02 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 10 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 10 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 10 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 02 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 10 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 03 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO Channel 11 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 03 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 03 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO Channel 11 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 03 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 11 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 11 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 03 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 03 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 11 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 11 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 05 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 05 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 13 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 13 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 05 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 05 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 05 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 05 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 13 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO Channel 13 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 13 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO Channel 13 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 06 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 06 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 14 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 06 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 06 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 14 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO Channel 14 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO Channel 14 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 07 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO Channel 15 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 07 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO Channel 15 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:algo-1-5y4z8:127:741 [3] NCCL INFO comm 0x7f7cc0002fb0 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:algo-1-5y4z8:121:740 [1] NCCL INFO comm 0x7f00a4002fb0 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:algo-1-5y4z8:134:735 [7] NCCL INFO comm 0x7f1bc4002fb0 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:734 [0] NCCL INFO comm 0x7f787c002fb0 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:algo-1-5y4z8:124:738 [2] NCCL INFO comm 0x7f36d8002fb0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:algo-1-5y4z8:131:736 [5] NCCL INFO comm 0x7f2b28002fb0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:algo-1-5y4z8:133:737 [6] NCCL INFO comm 0x7f3908002fb0 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:algo-1-5y4z8:129:739 [4] NCCL INFO comm 0x7fb24c002fb0 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:algo-1-5y4z8:587:587 [0] NCCL INFO Launch mode Parallel\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stderr>:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stderr>:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stderr>:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stderr>:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stderr>:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stderr>:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stderr>:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  | Name | Type | Params\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:------------------------------\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:0 | net  | UNet | 31.0 M\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:------------------------------\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:31.0 M    Trainable params\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:0         Non-trainable params\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:31.0 M    Total params\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:62.089    Total estimated model params size (MB)\n",
      "Sanity Checking: 0it [00:00, ?it/s][1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  rank_zero_warn(\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:[2022-08-16 02:49:32.149 algo-1-5y4z8:131 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:[2022-08-16 02:49:32.149 algo-1-5y4z8:124 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:[2022-08-16 02:49:32.149 algo-1-5y4z8:121 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:[2022-08-16 02:49:32.150 algo-1-5y4z8:134 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:[2022-08-16 02:49:32.150 algo-1-5y4z8:133 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-08-16 02:49:32.150 algo-1-5y4z8:587 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:[2022-08-16 02:49:32.150 algo-1-5y4z8:129 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:[2022-08-16 02:49:32.153 algo-1-5y4z8:127 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stdout>:[2022-08-16 02:49:32.301 algo-1-5y4z8:121 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stdout>:[2022-08-16 02:49:32.301 algo-1-5y4z8:131 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stdout>:[2022-08-16 02:49:32.302 algo-1-5y4z8:133 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stdout>:[2022-08-16 02:49:32.302 algo-1-5y4z8:124 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:[2022-08-16 02:49:32.303 algo-1-5y4z8:587 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stdout>:[2022-08-16 02:49:32.305 algo-1-5y4z8:134 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stdout>:[2022-08-16 02:49:32.306 algo-1-5y4z8:129 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stdout>:[2022-08-16 02:49:32.311 algo-1-5y4z8:127 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Sanity Checking DataLoader 0:   0% 0/1 [00:00<?, ?it/s]<stdout>:\n",
      "Sanity Checking DataLoader 0: 100% 1/1 [00:05<00:00,  5.31s/it]:\n",
      "Sanity Checking DataLoader 0: 100% 1/1 [00:05<00:00,  5.31s/it]:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  rank_zero_warn(\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:  rank_zero_warn(\n",
      "Training: 0it [00:00, ?it/s] |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Training:   0% 0/4 [00:00<?, ?it/s][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 0:   0% 0/4 [00:00<?, ?it/s] [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:5,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:6,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:3,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:4,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:7,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:1,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:2,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  25% 1/4 [00:11<00:34, 11.48s/it]ank:0,algo-1]<stdout>:\n",
      "Epoch 0:  25% 1/4 [00:11<00:34, 11.48s/it, loss=2.99, v_num=0]>:\n",
      "Epoch 0:  50% 2/4 [00:12<00:12,  6.23s/it, loss=2.99, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 0:  50% 2/4 [00:12<00:12,  6.23s/it, loss=2.84, v_num=0]>:\n",
      "Epoch 0:  75% 3/4 [00:20<00:06,  6.99s/it, loss=2.84, v_num=0]>:\n",
      "Epoch 0:  75% 3/4 [00:20<00:06,  6.99s/it, loss=2.75, v_num=0]>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation: 0it [00:00, ?it/s][1,mpirank:0,algo-1]<stdout>:\u001b[A>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[Airank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0: 100% 1/1 [00:00<00:00, 77.77it/s]\u001b[At>:\n",
      "Epoch 0: 100% 4/4 [00:21<00:00,  5.30s/it, loss=2.75, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 0: 100% 4/4 [00:21<00:00,  5.32s/it, loss=2.75, v_num=0]>:\n",
      "                                                          \u001b[At>:\n",
      "Epoch 0: 100% 4/4 [00:21<00:00,  5.32s/it, loss=2.75, v_num=0]>:\n",
      "Epoch 1:   0% 0/4 [00:00<?, ?it/s, loss=2.75, v_num=0]        >:\n",
      "Epoch 1:  25% 1/4 [00:00<00:02,  1.15it/s, loss=2.75, v_num=0]>:\n",
      "Epoch 1:  25% 1/4 [00:00<00:02,  1.15it/s, loss=2.65, v_num=0]>:\n",
      "Epoch 1:  50% 2/4 [00:01<00:01,  1.18it/s, loss=2.65, v_num=0]>:\n",
      "Epoch 1:  50% 2/4 [00:01<00:01,  1.18it/s, loss=2.53, v_num=0]>:\n",
      "Epoch 1:  75% 3/4 [00:02<00:00,  1.43it/s, loss=2.53, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 1:  75% 3/4 [00:02<00:00,  1.43it/s, loss=2.48, v_num=0]>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation:   0% 0/1 [00:00<?, ?it/s][1,mpirank:0,algo-1]<stdout>:\u001b[A[1,mpirank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0: 100% 1/1 [00:00<00:00, 80.15it/s]\u001b[At>:\n",
      "Epoch 1: 100% 4/4 [00:02<00:00,  1.73it/s, loss=2.48, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 1: 100% 4/4 [00:02<00:00,  1.67it/s, loss=2.48, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "                                                          \u001b[At>:\n",
      "Epoch 1: 100% 4/4 [00:02<00:00,  1.67it/s, loss=2.48, v_num=0]>:\n",
      "Epoch 2:   0% 0/4 [00:00<?, ?it/s, loss=2.48, v_num=0]        >:\n",
      "Epoch 2:  25% 1/4 [00:01<00:03,  1.20s/it, loss=2.48, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 2:  25% 1/4 [00:01<00:03,  1.20s/it, loss=2.42, v_num=0]>:\n",
      "Epoch 2:  50% 2/4 [00:02<00:02,  1.04s/it, loss=2.42, v_num=0]>:\n",
      "Epoch 2:  50% 2/4 [00:02<00:02,  1.04s/it, loss=2.36, v_num=0]>:\n",
      "Epoch 2:  75% 3/4 [00:02<00:00,  1.20it/s, loss=2.36, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 2:  75% 3/4 [00:02<00:00,  1.20it/s, loss=2.31, v_num=0]>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[Airank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0: 100% 1/1 [00:00<00:00, 80.54it/s]\u001b[At>:\n",
      "Epoch 2: 100% 4/4 [00:02<00:00,  1.47it/s, loss=2.31, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 2: 100% 4/4 [00:02<00:00,  1.43it/s, loss=2.31, v_num=0]>:\n",
      "                                                          \u001b[At>:\n",
      "Epoch 2: 100% 4/4 [00:02<00:00,  1.43it/s, loss=2.31, v_num=0]>:\n",
      "Epoch 3:   0% 0/4 [00:00<?, ?it/s, loss=2.31, v_num=0]        >:\n",
      "Epoch 3:  25% 1/4 [00:00<00:02,  1.21it/s, loss=2.31, v_num=0]>:\n",
      "Epoch 3:  25% 1/4 [00:00<00:02,  1.21it/s, loss=2.25, v_num=0]>:\n",
      "Epoch 3:  50% 2/4 [00:01<00:01,  1.18it/s, loss=2.25, v_num=0]>:\n",
      "Epoch 3:  50% 2/4 [00:01<00:01,  1.18it/s, loss=2.23, v_num=0]>:\n",
      "Epoch 3:  75% 3/4 [00:02<00:00,  1.43it/s, loss=2.23, v_num=0]>:\n",
      "Epoch 3:  75% 3/4 [00:02<00:00,  1.43it/s, loss=2.2, v_num=0]t>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation:   0% 0/1 [00:00<?, ?it/s][1,mpirank:0,algo-1]<stdout>:\u001b[A\n",
      "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0: 100% 1/1 [00:00<00:00, 86.40it/s]\u001b[At>:\n",
      "Epoch 3: 100% 4/4 [00:02<00:00,  1.74it/s, loss=2.2, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 3: 100% 4/4 [00:02<00:00,  1.67it/s, loss=2.2, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "                                                          [1,mpirank:0,algo-1]<stdout>:\u001b[A\n",
      "Epoch 3: 100% 4/4 [00:02<00:00,  1.67it/s, loss=2.2, v_num=0]t>:\n",
      "Epoch 4:   0% 0/4 [00:00<?, ?it/s, loss=2.2, v_num=0]        t>:\n",
      "Epoch 4:  25% 1/4 [00:00<00:02,  1.23it/s, loss=2.2, v_num=0]t>:\n",
      "Epoch 4:  25% 1/4 [00:00<00:02,  1.23it/s, loss=2.16, v_num=0]>:\n",
      "Epoch 4:  50% 2/4 [00:01<00:01,  1.19it/s, loss=2.16, v_num=0]>:\n",
      "Epoch 4:  50% 2/4 [00:01<00:01,  1.19it/s, loss=2.12, v_num=0]>:\n",
      "Epoch 4:  75% 3/4 [00:02<00:00,  1.44it/s, loss=2.12, v_num=0]>:\n",
      "Epoch 4:  75% 3/4 [00:02<00:00,  1.44it/s, loss=2.09, v_num=0]>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation: 0it [00:00, ?it/s][1,mpirank:0,algo-1]<stdout>:\u001b[A>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[Airank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0: 100% 1/1 [00:00<00:00, 86.06it/s]\u001b[At>:\n",
      "Epoch 4: 100% 4/4 [00:02<00:00,  1.75it/s, loss=2.09, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 4: 100% 4/4 [00:02<00:00,  1.67it/s, loss=2.09, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\u001b[A\n",
      "Epoch 4: 100% 4/4 [00:02<00:00,  1.67it/s, loss=2.09, v_num=0]>:\n",
      "Epoch 5:   0% 0/4 [00:00<?, ?it/s, loss=2.09, v_num=0]        >:\n",
      "Epoch 5:  25% 1/4 [00:00<00:02,  1.21it/s, loss=2.09, v_num=0]>:\n",
      "Epoch 5:  25% 1/4 [00:00<00:02,  1.21it/s, loss=2.06, v_num=0]>:\n",
      "Epoch 5:  50% 2/4 [00:01<00:01,  1.18it/s, loss=2.06, v_num=0]>:\n",
      "Epoch 5:  50% 2/4 [00:01<00:01,  1.18it/s, loss=2.04, v_num=0]>:\n",
      "Epoch 5:  75% 3/4 [00:02<00:00,  1.43it/s, loss=2.04, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 5:  75% 3/4 [00:02<00:00,  1.43it/s, loss=2.01, v_num=0]>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation: 0it [00:00, ?it/s]\u001b[Am [1,mpirank:0,algo-1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation:   0% 0/1 [00:00<?, ?it/s]\u001b[A[1,mpirank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A1]<stdout>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m [1,mpirank:0,algo-1]<stdout>:\n",
      "Validation DataLoader 0: 100% 1/1 [00:00<00:00, 85.78it/s]\u001b[At>:\n",
      "Epoch 5: 100% 4/4 [00:02<00:00,  1.74it/s, loss=2.01, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "Epoch 5: 100% 4/4 [00:02<00:00,  1.67it/s, loss=2.01, v_num=0][1,mpirank:0,algo-1]<stdout>:\n",
      "                                                          \u001b[At>:\n",
      "Epoch 5: 100% 4/4 [00:02<00:00,  1.67it/s, loss=2.01, v_num=0]>:\n",
      "Epoch 5: 100% 4/4 [00:03<00:00,  1.16it/s, loss=2.01, v_num=0]>:\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:50:19,725 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:50:19,725 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 |\u001b[0m 2022-08-16 02:50:19,726 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36m5jfl2hnhwa-algo-1-5y4z8 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\"batch_size\": 8}\n",
    "enable_local_mode_training = True\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = \"local_gpu\"\n",
    "    inputs = {\"data_path\": f\"file:///home/ec2-user/SageMaker/amazon-sagemaker-pytorch-lightning-distributed-training/data_semantics\"}\n",
    "else:\n",
    "    train_instance_type = \"ml.g4dn.12xlarge\"\n",
    "    inputs = {\"data_path\": data_url}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"semantic_segmentation_single.py\",\n",
    "    \"source_dir\": \"code\",\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"pytorch-lightning\",\n",
    "    \"image_uri\": \"570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-custom:1.12.0-gpu-py38-cu113-ubuntu20.04-sagemaker-2.6.0-numproc\",\n",
    "    \"py_version\": \"py3\",\n",
    "    \"distribution\": {\"pytorchddp\":{\"enabled\": True}},\n",
    "}\n",
    "\n",
    "estimator = PyTorch(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d30b47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-16 06:39:31 Starting - Starting the training job.........\n",
      "2022-08-16 06:40:38 Starting - Preparing the instances for training.........\n",
      "2022-08-16 06:42:15 Downloading - Downloading input data...\n",
      "2022-08-16 06:42:51 Training - Downloading the training image..........................\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:20,099 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:20,180 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:20,183 sagemaker_pytorch_container.training INFO     Pytorch_ddp_enabled is:\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:20,183 sagemaker_pytorch_container.training INFO     True\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:20,183 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:20,183 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:19,981 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:20,058 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:20,061 sagemaker_pytorch_container.training INFO     Pytorch_ddp_enabled is:\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:20,062 sagemaker_pytorch_container.training INFO     True\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:20,062 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:20,062 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:20,259 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:20,337 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:20,341 sagemaker_pytorch_container.training INFO     Pytorch_ddp_enabled is:\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:20,341 sagemaker_pytorch_container.training INFO     True\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:20,341 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:20,341 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:20,908 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\n",
      "2022-08-16 06:47:13 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:20,794 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:20,577 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.12.0+cu113)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.13.0+cu113)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.6.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (9.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2022.5.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: torchmetrics>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.9.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.3.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (21.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning->-r requirements.txt (line 3)) (3.0.9)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.1.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.4.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.6)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.47.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.19.4)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (63.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.6.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.37.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.1.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.10)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (5.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.12.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (6.0.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.12.0+cu113)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.13.0+cu113)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.6.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (9.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (21.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchmetrics>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.9.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2022.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning->-r requirements.txt (line 3)) (3.0.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.47.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.19.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (63.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.37.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (5.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (21.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (6.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.12.0+cu113)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.13.0+cu113)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.6.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (9.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2022.5.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (21.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.3.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: torchmetrics>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.9.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning->-r requirements.txt (line 3)) (3.0.9)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.1.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.4.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.6.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (63.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.6)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.19.4)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.37.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.47.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.1.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.10)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (5.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.12.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (21.4.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (6.0.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:22,780 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:22,857 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:22,860 sagemaker_pytorch_container.training INFO     Pytorch_ddp_enabled is:\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:22,860 sagemaker_pytorch_container.training INFO     True\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:22,860 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:22,860 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (21.4.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[32mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[32m[notice] A new release of pip available: 22.2 -> 22.2.2\u001b[0m\n",
      "\u001b[32m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip available: 22.2 -> 22.2.2\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:22,884 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:22,884 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:23,040 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:23,040 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:23,045 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:23,047 sagemaker-training-toolkit INFO     Cannot connect to host algo-3 at port 22. Retrying...\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:23,047 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[36mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[36m[notice] A new release of pip available: 22.2 -> 22.2.2\u001b[0m\n",
      "\u001b[36m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,256 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,256 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:23,629 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,208 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,208 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,363 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,363 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,375 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,471 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,471 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,471 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,471 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:23,479 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:24,058 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:24,156 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:24,156 sagemaker-training-toolkit INFO     Can connect to host algo-3 at port 22\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:24,156 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:24,157 sagemaker-training-toolkit INFO     Worker algo-3 available for communication\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:24,157 sagemaker-training-toolkit INFO     Cannot connect to host algo-1 at port 22. Retrying...\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:24,157 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,412 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,412 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,422 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,491 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,491 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,491 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,491 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:23,498 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.12.0+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.13.0+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->-r requirements.txt (line 2)) (9.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2022.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchmetrics>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 3)) (21.3)\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:25,159 sagemaker-training-toolkit INFO     Cannot connect to host algo-1 at port 22. Retrying...\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:25,159 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning->-r requirements.txt (line 3)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.19.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (63.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.47.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.37.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (5.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:26,161 sagemaker-training-toolkit INFO     Cannot connect to host algo-1 at port 22. Retrying...\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:26,161 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2 -> 22.2.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,416 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,416 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,574 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,574 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,585 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,651 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,651 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,651 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,651 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:26,658 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,174 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,307 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,308 sagemaker-training-toolkit INFO     Can connect to host algo-1 at port 22\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,308 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,308 sagemaker-training-toolkit INFO     Worker algo-1 available for communication\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,319 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,420 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,420 sagemaker-training-toolkit INFO     Can connect to host algo-4 at port 22\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,420 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,420 sagemaker-training-toolkit INFO     Worker algo-4 available for communication\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,420 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,420 sagemaker-training-toolkit INFO     Host: ['algo-2', 'algo-3', 'algo-1', 'algo-4']\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,422 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,422 sagemaker-training-toolkit INFO     Env Hosts: ['algo-2', 'algo-3', 'algo-1', 'algo-4'] Hosts: ['algo-2:8', 'algo-3:8', 'algo-1:8', 'algo-4:8'] process_per_hosts: 8 num_processes: 32\u001b[0m\n",
      "\u001b[35m2022-08-16 06:47:27,502 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_pytorch_ddp_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"data_path\": \"/opt/ml/input/data/data_path\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 8\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"data_path\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\",\n",
      "                \"algo-4\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-lightning-2022-08-16-06-39-31-116\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-2\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-631450739534/pytorch-lightning-2022-08-16-06-39-31-116/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"semantic_segmentation\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"semantic_segmentation.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"batch_size\":8}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=semantic_segmentation.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_pytorch_ddp_enabled\":true}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-3\",\"algo-1\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"data_path\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"data_path\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-3\",\"algo-1\",\"algo-4\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-3\",\"algo-1\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=semantic_segmentation\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-631450739534/pytorch-lightning-2022-08-16-06-39-31-116/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{\"data_path\":\"/opt/ml/input/data/data_path\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-3\",\"algo-1\",\"algo-4\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[\"algo-2\",\"algo-3\",\"algo-1\",\"algo-4\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"hyperparameters\":{\"batch_size\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"data_path\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-3\",\"algo-1\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-lightning-2022-08-16-06-39-31-116\",\"log_level\":20,\"master_hostname\":\"algo-2\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-631450739534/pytorch-lightning-2022-08-16-06-39-31-116/source/sourcedir.tar.gz\",\"module_name\":\"semantic_segmentation\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-3\",\"algo-1\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"semantic_segmentation.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--batch_size\",\"8\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_DATA_PATH=/opt/ml/input/data/data_path\u001b[0m\n",
      "\u001b[35mSM_HP_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220722-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mmpirun --host algo-2:8,algo-3:8,algo-1:8,algo-4:8 -np 32 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-2 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /opt/conda/bin/python3.8 -m mpi4py semantic_segmentation.py --batch_size 8\u001b[0m\n",
      "\u001b[35mWarning: Permanently added 'algo-4,10.2.253.246' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[35mWarning: Permanently added 'algo-3,10.2.229.85' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[35mWarning: Permanently added 'algo-1,10.2.245.19' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:28,664 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=128, name='orted', status='sleeping', started='06:47:27')]\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:28,665 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=128, name='orted', status='sleeping', started='06:47:27')]\u001b[0m\n",
      "\u001b[34m2022-08-16 06:47:28,665 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=128, name='orted', status='sleeping', started='06:47:27')]\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:28,492 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=128, name='orted', status='sleeping', started='06:47:27')]\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:28,493 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=128, name='orted', status='sleeping', started='06:47:27')]\u001b[0m\n",
      "\u001b[32m2022-08-16 06:47:28,493 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=128, name='orted', status='sleeping', started='06:47:27')]\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:28,511 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=128, name='orted', status='sleeping', started='06:47:27')]\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:28,511 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=128, name='orted', status='sleeping', started='06:47:27')]\u001b[0m\n",
      "\u001b[36m2022-08-16 06:47:28,512 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=128, name='orted', status='sleeping', started='06:47:27')]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stderr>:Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stderr>:Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stderr>:Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stderr>:Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stderr>:Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stderr>:Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stderr>:Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stderr>:Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Using 16bit native Automatic Mixed Precision (AMP)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:GPU available: True, used: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:TPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:IPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:HPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stderr>:Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stderr>:Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stderr>:Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stderr>:Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stderr>:Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stderr>:Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stderr>:Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stderr>:Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/32\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:distributed_backend=nccl\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:All distributed processes registered. Starting with 32 processes\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:Missing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] NCCL INFO Bootstrap : Using eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:292 [1] NCCL INFO Bootstrap : Using eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:298 [2] NCCL INFO Bootstrap : Using eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:227 [4] NCCL INFO Bootstrap : Using eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:225 [6] NCCL INFO Bootstrap : Using eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:223 [7] NCCL INFO Bootstrap : Using eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:297 [3] NCCL INFO Bootstrap : Using eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:296 [5] NCCL INFO Bootstrap : Using eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:292 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:298 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:298 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:223 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:223 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:292 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:292 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:225 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:225 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:225 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:298 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:223 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:227 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:227 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:227 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:339 [1] NCCL INFO Bootstrap : Using eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:333 [4] NCCL INFO Bootstrap : Using eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:339 [6] NCCL INFO Bootstrap : Using eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:204 [6] NCCL INFO Bootstrap : Using eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:336 [7] NCCL INFO Bootstrap : Using eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:672 [0] NCCL INFO Bootstrap : Using eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:340 [3] NCCL INFO Bootstrap : Using eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:296 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:296 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:296 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:297 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:297 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:297 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:202 [3] NCCL INFO Bootstrap : Using eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:332 [5] NCCL INFO Bootstrap : Using eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:336 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:336 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:672 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:672 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:339 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:339 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:204 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:204 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:340 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:340 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:340 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:336 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:672 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:339 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:204 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:333 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:333 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:333 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:672 [0] NCCL INFO Bootstrap : Using eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:339 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:339 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:339 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:332 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:202 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:332 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:202 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:332 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:202 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:672 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:672 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:672 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:340 [4] NCCL INFO Bootstrap : Using eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:340 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:340 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:340 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:335 [2] NCCL INFO Bootstrap : Using eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:335 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:335 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:335 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:202 [5] NCCL INFO Bootstrap : Using eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:202 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:202 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:202 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:338 [2] NCCL INFO Bootstrap : Using eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:338 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:338 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:338 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:273 [3] NCCL INFO Bootstrap : Using eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:273 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:273 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:273 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:202 [4] NCCL INFO Bootstrap : Using eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:202 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:202 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:202 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:339 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:339 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:672 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:672 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:332 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:332 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:340 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:340 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:202 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:202 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:335 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:335 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:672 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:332 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:339 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:335 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:202 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:340 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:335 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:335 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:672 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:672 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:202 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:202 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:332 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:332 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:339 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:339 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:340 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:340 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:298 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:298 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:298 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:298 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:298 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:292 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:292 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:292 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:225 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:225 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:292 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:223 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:225 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:292 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:223 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:223 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:225 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:225 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:223 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:227 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:223 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:227 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:227 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:227 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:227 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:297 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:297 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:296 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:296 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:297 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:296 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:297 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:297 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:296 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.195.145<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:296 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:338 [1] NCCL INFO Bootstrap : Using eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:338 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:338 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:338 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:339 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:339 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:340 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:340 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:336 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:336 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:333 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:333 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:204 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:204 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:672 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:672 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:339 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:340 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:336 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:204 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:672 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:333 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:204 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:204 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:339 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:339 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:340 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:340 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:672 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:672 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:336 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:336 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:333 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:333 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:202 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:202 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:202 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:202 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:202 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:202 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:202 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:273 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:273 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:202 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:202 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:202 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:273 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:273 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:273 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:338 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:338 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:338 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:338 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:338 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:338 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:338 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:338 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:338 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.229.85<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:338 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:204 [7] NCCL INFO Bootstrap : Using eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:204 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:204 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:204 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:204 [2] NCCL INFO Bootstrap : Using eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:204 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:204 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:204 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:204 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:204 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:204 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:204 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.253.246<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:204 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:204 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:204 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:204 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:204 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:204 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:269 [1] NCCL INFO Bootstrap : Using eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:269 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:269 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:269 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:271 [5] NCCL INFO Bootstrap : Using eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:271 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:271 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:271 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:269 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:269 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:269 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:269 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:269 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:271 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:271 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:271 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:271 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:271 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:276 [7] NCCL INFO Bootstrap : Using eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:276 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:276 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:276 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:276 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:276 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:276 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:276 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:276 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:672 [0] NCCL INFO Bootstrap : Using eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:672 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:672 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:672 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:672 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:672 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:672 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:672 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:672 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:277 [6] NCCL INFO Bootstrap : Using eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:277 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:277 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:277 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:277 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:277 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:277 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:277 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.245.19<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:277 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12  16  19  18  17\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12  16  19  18  17\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Trees [0] 3/16/-1->0->-1 [1] 3/-1/-1->0->8\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Trees [0] 31/-1/-1->30->29 [1] 31/-1/-1->30->29\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Trees [0] -1/-1/-1->28->31 [1] -1/-1/-1->28->31\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Trees [0] 25/-1/-1->26->27 [1] 25/-1/-1->26->27\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Trees [0] 26/-1/-1->27->24 [1] 26/-1/-1->27->24\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Trees [0] 30/-1/-1->29->25 [1] 30/-1/-1->29->25\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Trees [0] 28/-1/-1->31->30 [1] 28/-1/-1->31->30\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Trees [0] 14/-1/-1->13->9 [1] 14/-1/-1->13->9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Trees [0] 12/-1/-1->15->14 [1] 12/-1/-1->15->14\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Trees [0] 13/-1/-1->9->10 [1] 13/-1/-1->9->10\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Trees [0] 17/-1/-1->18->19 [1] 17/-1/-1->18->19\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Trees [0] 10/-1/-1->11->8 [1] 10/16/-1->11->8\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Trees [0] 18/8/-1->19->16 [1] 18/-1/-1->19->16\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Trees [0] 11/-1/-1->8->19 [1] 11/0/-1->8->24\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Trees [0] 21/-1/-1->17->18 [1] 21/-1/-1->17->18\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Trees [0] 9/-1/-1->10->11 [1] 9/-1/-1->10->11\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Trees [0] 19/24/-1->16->0 [1] 19/-1/-1->16->11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Trees [0] -1/-1/-1->12->15 [1] -1/-1/-1->12->15\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Trees [0] -1/-1/-1->20->23 [1] -1/-1/-1->20->23\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Trees [0] 22/-1/-1->21->17 [1] 22/-1/-1->21->17\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/-1/-1->22->21\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Trees [0] 20/-1/-1->23->22 [1] 20/-1/-1->23->22\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Trees [0] 27/-1/-1->24->16 [1] 27/8/-1->24->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Trees [0] 29/-1/-1->25->26 [1] 29/-1/-1->25->26\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 00 : 16[170] -> 19[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Channel 00 : 29[1c0] -> 30[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Channel 00 : 17[180] -> 21[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Channel 00 : 25[180] -> 29[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Channel 00 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Channel 00 : 21[1c0] -> 22[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 00 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 00 : 24[170] -> 27[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 01 : 16[170] -> 19[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Channel 01 : 17[180] -> 21[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Channel 01 : 29[1c0] -> 30[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Channel 00 : 28[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Channel 01 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Channel 01 : 21[1c0] -> 22[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Channel 01 : 25[180] -> 29[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 01 : 24[170] -> 27[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Channel 00 : 30[1d0] -> 31[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 01 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Channel 00 : 20[1b0] -> 24[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Channel 00 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Channel 00 : 26[190] -> 25[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Channel 00 : 18[190] -> 17[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Channel 01 : 30[1d0] -> 31[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Channel 00 : 12[1b0] -> 16[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Channel 00 : 22[1d0] -> 23[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Channel 01 : 18[190] -> 17[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Channel 01 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Channel 01 : 22[1d0] -> 23[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Channel 00 : 19[1a0] -> 18[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Channel 01 : 26[190] -> 25[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Channel 00 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Channel 01 : 19[1a0] -> 18[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Channel 01 : 28[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Channel 01 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Channel 00 : 27[1a0] -> 26[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Channel 01 : 20[1b0] -> 24[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Channel 00 : 31[1e0] -> 28[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Channel 01 : 27[1a0] -> 26[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 00 : 12[1b0] -> 16[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 00 : 28[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Channel 00 : 23[1e0] -> 20[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Channel 01 : 12[1b0] -> 16[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Channel 00 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Channel 01 : 31[1e0] -> 28[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Channel 01 : 23[1e0] -> 20[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Channel 01 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 00 : 20[1b0] -> 24[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Channel 00 : 30[1d0] -> 29[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Channel 00 : 29[1c0] -> 25[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Channel 00 : 21[1c0] -> 17[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 01 : 12[1b0] -> 16[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Channel 00 : 22[1d0] -> 21[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 01 : 28[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Channel 00 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Channel 01 : 21[1c0] -> 17[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Channel 01 : 30[1d0] -> 29[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Channel 01 : 29[1c0] -> 25[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Channel 01 : 22[1d0] -> 21[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Channel 01 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Channel 00 : 17[180] -> 18[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Channel 01 : 17[180] -> 18[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Channel 00 : 25[180] -> 26[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 01 : 20[1b0] -> 24[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Channel 00 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Channel 01 : 25[180] -> 26[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Channel 01 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Channel 00 : 18[190] -> 19[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Channel 01 : 18[190] -> 19[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Channel 00 : 26[190] -> 27[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Channel 00 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Channel 01 : 26[190] -> 27[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Channel 01 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 01 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Channel 00 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Channel 00 : 27[1a0] -> 24[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Channel 00 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Channel 00 : 28[1b0] -> 31[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 01 : 11[1a0] -> 16[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Channel 01 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Channel 00 : 8[170] -> 19[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 00 : 16[170] -> 24[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Channel 01 : 27[1a0] -> 24[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 01 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Channel 01 : 28[1b0] -> 31[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Channel 00 : 18[190] -> 20[1b0] via P2P/indirect/16[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Channel 00 : 20[1b0] -> 23[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Channel 00 : 26[190] -> 28[1b0] via P2P/indirect/24[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Channel 01 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Channel 01 : 17[180] -> 20[1b0] via P2P/indirect/16[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Channel 01 : 11[1a0] -> 16[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Channel 01 : 20[1b0] -> 23[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Channel 00 : 10[190] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Channel 01 : 25[180] -> 28[1b0] via P2P/indirect/24[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Channel 00 : 31[1e0] -> 30[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Channel 00 : 23[1e0] -> 22[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Channel 01 : 31[1e0] -> 30[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Channel 01 : 23[1e0] -> 22[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 00 : 16[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Channel 01 : 9[180] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 00 : 8[170] -> 19[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Channel 01 : 16[170] -> 11[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 00 : 16[170] -> 24[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 00 : 0[170] -> 16[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 01 : 24[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Channel 00 : 19[1a0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 00 : 0[170] -> 16[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 01 : 8[170] -> 24[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 01 : 8[170] -> 24[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 00 : 16[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 01 : 24[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 01 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 00 : 24[170] -> 16[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 00 : 19[1a0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 00 : 24[170] -> 16[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Channel 00 : 19[1a0] -> 16[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Channel 01 : 19[1a0] -> 16[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Channel 01 : 27[1a0] -> 28[1b0] via P2P/indirect/24[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 01 : 24[170] -> 29[1c0] via P2P/indirect/25[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Channel 00 : 27[1a0] -> 29[1c0] via P2P/indirect/25[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 01 : 16[170] -> 11[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 01 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO Channel 01 : 27[1a0] -> 30[1d0] via P2P/indirect/31[1e0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Channel 00 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Channel 01 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Channel 01 : 26[190] -> 29[1c0] via P2P/indirect/25[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 01 : 16[170] -> 21[1c0] via P2P/indirect/17[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 01 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 01 : 8[170] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Channel 01 : 19[1a0] -> 20[1b0] via P2P/indirect/16[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Channel 01 : 11[1a0] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Channel 01 : 28[1b0] -> 25[180] via P2P/indirect/29[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Channel 01 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Channel 01 : 25[180] -> 30[1d0] via P2P/indirect/29[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Channel 00 : 19[1a0] -> 21[1c0] via P2P/indirect/17[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Channel 00 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Channel 00 : 11[1a0] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO Channel 01 : 26[190] -> 31[1e0] via P2P/indirect/30[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO Channel 01 : 19[1a0] -> 22[1d0] via P2P/indirect/23[1e0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO Channel 01 : 11[1a0] -> 14[1d0] via P2P/indirect/15[1e0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO Channel 01 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Channel 01 : 18[190] -> 21[1c0] via P2P/indirect/17[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO Channel 00 : 25[180] -> 31[1e0] via P2P/indirect/27[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Channel 01 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Channel 01 : 10[190] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO Channel 01 : 18[190] -> 23[1e0] via P2P/indirect/22[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO Channel 01 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Channel 01 : 29[1c0] -> 24[170] via P2P/indirect/28[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO Channel 01 : 10[190] -> 15[1e0] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Channel 01 : 12[1b0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 00 : 24[170] -> 30[1d0] via P2P/indirect/28[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Channel 01 : 20[1b0] -> 17[180] via P2P/indirect/21[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Channel 01 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Channel 01 : 17[180] -> 22[1d0] via P2P/indirect/21[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Channel 00 : 30[1d0] -> 24[170] via P2P/indirect/28[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO Channel 01 : 24[170] -> 31[1e0] via P2P/indirect/28[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Channel 01 : 9[180] -> 14[1d0] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Channel 01 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 00 : 8[170] -> 14[1d0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO Channel 00 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Channel 01 : 31[1e0] -> 24[170] via P2P/indirect/28[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 00 : 16[170] -> 22[1d0] via P2P/indirect/20[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Channel 01 : 13[1c0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO Channel 00 : 17[180] -> 23[1e0] via P2P/indirect/19[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Channel 01 : 21[1c0] -> 16[170] via P2P/indirect/20[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Channel 00 : 31[1e0] -> 25[180] via P2P/indirect/29[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO Channel 00 : 9[180] -> 15[1e0] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Channel 01 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO Channel 01 : 31[1e0] -> 26[190] via P2P/indirect/27[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO Channel 01 : 16[170] -> 23[1e0] via P2P/indirect/20[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO Channel 01 : 8[170] -> 15[1e0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Channel 01 : 30[1d0] -> 25[180] via P2P/indirect/29[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Channel 00 : 22[1d0] -> 16[170] via P2P/indirect/20[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Channel 00 : 14[1d0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Channel 01 : 23[1e0] -> 16[170] via P2P/indirect/20[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Channel 01 : 15[1e0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO Channel 01 : 30[1d0] -> 27[1a0] via P2P/indirect/26[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Channel 00 : 23[1e0] -> 17[180] via P2P/indirect/21[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Channel 00 : 15[1e0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Channel 01 : 29[1c0] -> 26[190] via P2P/indirect/25[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO Channel 01 : 23[1e0] -> 18[190] via P2P/indirect/19[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO Channel 01 : 15[1e0] -> 10[190] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Channel 01 : 14[1d0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Channel 00 : 28[1b0] -> 26[190] via P2P/indirect/30[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Channel 01 : 22[1d0] -> 17[180] via P2P/indirect/21[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO Channel 00 : 29[1c0] -> 27[1a0] via P2P/indirect/25[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO Channel 01 : 28[1b0] -> 27[1a0] via P2P/indirect/24[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Channel 01 : 13[1c0] -> 10[190] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Channel 01 : 21[1c0] -> 18[190] via P2P/indirect/17[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO Channel 01 : 14[1d0] -> 11[1a0] via P2P/indirect/10[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO Channel 01 : 22[1d0] -> 19[1a0] via P2P/indirect/18[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stdout>:algo-4:338:823 [1] NCCL INFO comm 0x7fc6a4002fb0 rank 25 nranks 32 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stdout>:algo-4:332:818 [5] NCCL INFO comm 0x7f9200002fb0 rank 29 nranks 32 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stdout>:algo-4:204:824 [7] NCCL INFO comm 0x7fe0dc002fb0 rank 31 nranks 32 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stdout>:algo-4:340:822 [4] NCCL INFO comm 0x7f012c002fb0 rank 28 nranks 32 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stdout>:algo-4:335:820 [2] NCCL INFO comm 0x7f1970002fb0 rank 26 nranks 32 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stdout>:algo-4:202:821 [3] NCCL INFO comm 0x7fd4e4002fb0 rank 27 nranks 32 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stdout>:algo-4:672:817 [0] NCCL INFO comm 0x7f8a78002fb0 rank 24 nranks 32 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stdout>:algo-4:339:819 [6] NCCL INFO comm 0x7f4cb8002fb0 rank 30 nranks 32 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO Channel 00 : 13[1c0] -> 11[1a0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Channel 00 : 12[1b0] -> 10[190] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO Channel 00 : 21[1c0] -> 19[1a0] via P2P/indirect/17[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Channel 00 : 20[1b0] -> 18[190] via P2P/indirect/22[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO Channel 01 : 12[1b0] -> 11[1a0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO Channel 01 : 20[1b0] -> 19[1a0] via P2P/indirect/16[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-3:333:821 [4] NCCL INFO comm 0x7f3d94002fb0 rank 12 nranks 32 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-3:204:817 [6] NCCL INFO comm 0x7fe168002fb0 rank 14 nranks 32 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-3:340:818 [3] NCCL INFO comm 0x7f8e68002fb0 rank 11 nranks 32 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-3:336:820 [7] NCCL INFO comm 0x7f373c002fb0 rank 15 nranks 32 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-3:672:822 [0] NCCL INFO comm 0x7f2afc002fb0 rank 8 nranks 32 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-3:202:823 [5] NCCL INFO comm 0x7feec8002fb0 rank 13 nranks 32 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-3:338:824 [2] NCCL INFO comm 0x7f903c002fb0 rank 10 nranks 32 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-3:339:819 [1] NCCL INFO comm 0x7fd464002fb0 rank 9 nranks 32 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stdout>:algo-1:277:824 [6] NCCL INFO comm 0x7fe41c002fb0 rank 22 nranks 32 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stdout>:algo-1:269:820 [1] NCCL INFO comm 0x7fd2b0002fb0 rank 17 nranks 32 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stdout>:algo-1:271:821 [5] NCCL INFO comm 0x7fa66c002fb0 rank 21 nranks 32 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stdout>:algo-1:204:819 [2] NCCL INFO comm 0x7f26fc002fb0 rank 18 nranks 32 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stdout>:algo-1:202:817 [4] NCCL INFO comm 0x7f193c002fb0 rank 20 nranks 32 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stdout>:algo-1:672:823 [0] NCCL INFO comm 0x7fc658002fb0 rank 16 nranks 32 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stdout>:algo-1:273:818 [3] NCCL INFO comm 0x7fe25c002fb0 rank 19 nranks 32 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stdout>:algo-1:276:822 [7] NCCL INFO comm 0x7ff23c002fb0 rank 23 nranks 32 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 00 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Channel 00 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO Channel 01 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Channel 01 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Channel 00 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO Channel 01 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Channel 01 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Channel 01 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO Channel 01 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO Channel 00 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Channel 00 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO Channel 01 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:225:843 [6] NCCL INFO comm 0x7fe55c002fb0 rank 6 nranks 32 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:840 [0] NCCL INFO comm 0x7f9964002fb0 rank 0 nranks 32 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:297:846 [3] NCCL INFO comm 0x7fe654002fb0 rank 3 nranks 32 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:298:841 [2] NCCL INFO comm 0x7f3564002fb0 rank 2 nranks 32 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:223:844 [7] NCCL INFO comm 0x7efdc8002fb0 rank 7 nranks 32 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:227:845 [4] NCCL INFO comm 0x7fa9b8002fb0 rank 4 nranks 32 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:292:842 [1] NCCL INFO comm 0x7f0b98002fb0 rank 1 nranks 32 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:296:847 [5] NCCL INFO comm 0x7fe488002fb0 rank 5 nranks 32 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:693:693 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stderr>:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stderr>:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stderr>:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stderr>:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stderr>:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stderr>:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stderr>:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stderr>:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stderr>:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stderr>:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stderr>:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stderr>:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stderr>:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stderr>:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stderr>:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stderr>:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  | Name | Type | Params\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:0 | net  | UNet | 31.0 M\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:31.0 M    Trainable params\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:0         Non-trainable params\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:31.0 M    Total params\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:62.089    Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Sanity Checking: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  rank_zero_warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Sanity Checking:   0% 0/1 [00:00<?, ?it/s]#015Sanity Checking DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Sanity Checking DataLoader 0: 100% 1/1 [00:05<00:00,  5.26s/it]#015Sanity Checking DataLoader 0: 100% 1/1 [00:05<00:00,  5.26s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  rank_zero_warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  rank_zero_warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Training: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Training:   0% 0/2 [00:00<?, ?it/s][1,mpirank:0,algo-1]<stdout>:#015Epoch 0:   0% 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:30,algo-4]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:19,algo-3]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:31,algo-4]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:28,algo-4]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:20,algo-3]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:25,algo-4]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:24,algo-4]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:27,algo-4]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:29,algo-4]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:26,algo-4]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:23,algo-3]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:21,algo-3]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:22,algo-3]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:16,algo-3]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:18,algo-3]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:17,algo-3]<stderr>:[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 0:  50% 1/2 [00:09<00:09,  9.57s/it]#015Epoch 0:  50% 1/2 [00:09<00:09,  9.57s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 0:  50% 1/2 [00:09<00:09,  9.57s/it, loss=2.93, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 77.40it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 0: 100% 2/2 [00:09<00:00,  4.83s/it, loss=2.93, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 0: 100% 2/2 [00:09<00:00,  4.87s/it, loss=2.93, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 0: 100% 2/2 [00:09<00:00,  4.87s/it, loss=2.93, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 0:   0% 0/2 [00:00<?, ?it/s, loss=2.93, v_num=0]        #015Epoch 1:   0% 0/2 [00:00<?, ?it/s, loss=2.93, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 1:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.93, v_num=0][1,mpirank:0,algo-1]<stdout>:#015Epoch 1:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.93, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 1:  50% 1/2 [00:00<00:00,  1.85it/s, loss=2.71, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 95.30it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 1: 100% 2/2 [00:00<00:00,  3.17it/s, loss=2.71, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 1: 100% 2/2 [00:00<00:00,  2.96it/s, loss=2.71, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 1: 100% 2/2 [00:00<00:00,  2.95it/s, loss=2.71, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 1:   0% 0/2 [00:00<?, ?it/s, loss=2.71, v_num=0]        #015Epoch 2:   0% 0/2 [00:00<?, ?it/s, loss=2.71, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 2:  50% 1/2 [00:00<00:00,  1.89it/s, loss=2.71, v_num=0][1,mpirank:0,algo-1]<stdout>:#015Epoch 2:  50% 1/2 [00:00<00:00,  1.89it/s, loss=2.71, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 2:  50% 1/2 [00:00<00:00,  1.89it/s, loss=2.58, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 96.53it/s]#033[A[1,mpirank:0,algo-1]<stdout>:#015Epoch 2: 100% 2/2 [00:00<00:00,  3.22it/s, loss=2.58, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 2: 100% 2/2 [00:00<00:00,  2.96it/s, loss=2.58, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 2: 100% 2/2 [00:00<00:00,  2.95it/s, loss=2.58, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 2:   0% 0/2 [00:00<?, ?it/s, loss=2.58, v_num=0]        #015Epoch 3:   0% 0/2 [00:00<?, ?it/s, loss=2.58, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 3:  50% 1/2 [00:00<00:00,  1.91it/s, loss=2.58, v_num=0]#015Epoch 3:  50% 1/2 [00:00<00:00,  1.91it/s, loss=2.58, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 3:  50% 1/2 [00:00<00:00,  1.91it/s, loss=2.43, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 94.05it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 3: 100% 2/2 [00:00<00:00,  3.25it/s, loss=2.43, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 3: 100% 2/2 [00:00<00:00,  3.00it/s, loss=2.43, v_num=0][1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 3: 100% 2/2 [00:00<00:00,  2.99it/s, loss=2.43, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 3:   0% 0/2 [00:00<?, ?it/s, loss=2.43, v_num=0]        #015Epoch 4:   0% 0/2 [00:00<?, ?it/s, loss=2.43, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 4:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.43, v_num=0][1,mpirank:0,algo-1]<stdout>:#015Epoch 4:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.43, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 4:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.35, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 94.42it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 4: 100% 2/2 [00:00<00:00,  3.17it/s, loss=2.35, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 4: 100% 2/2 [00:00<00:00,  2.89it/s, loss=2.35, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 4: 100% 2/2 [00:00<00:00,  2.88it/s, loss=2.35, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 4:   0% 0/2 [00:00<?, ?it/s, loss=2.35, v_num=0]        [1,mpirank:0,algo-1]<stdout>:#015Epoch 5:   0% 0/2 [00:00<?, ?it/s, loss=2.35, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 5:  50% 1/2 [00:00<00:00,  1.90it/s, loss=2.35, v_num=0][1,mpirank:0,algo-1]<stdout>:#015Epoch 5:  50% 1/2 [00:00<00:00,  1.90it/s, loss=2.35, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 5:  50% 1/2 [00:00<00:00,  1.90it/s, loss=2.26, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 95.62it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 5: 100% 2/2 [00:00<00:00,  3.23it/s, loss=2.26, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 5: 100% 2/2 [00:00<00:00,  3.00it/s, loss=2.26, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 5: 100% 2/2 [00:00<00:00,  2.99it/s, loss=2.26, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 5:   0% 0/2 [00:00<?, ?it/s, loss=2.26, v_num=0]        #015Epoch 6:   0% 0/2 [00:00<?, ?it/s, loss=2.26, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 6:  50% 1/2 [00:00<00:00,  1.87it/s, loss=2.26, v_num=0][1,mpirank:0,algo-1]<stdout>:#015Epoch 6:  50% 1/2 [00:00<00:00,  1.87it/s, loss=2.26, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 6:  50% 1/2 [00:00<00:00,  1.87it/s, loss=2.2, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 94.68it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 6: 100% 2/2 [00:00<00:00,  3.18it/s, loss=2.2, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 6: 100% 2/2 [00:00<00:00,  2.96it/s, loss=2.2, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 6: 100% 2/2 [00:00<00:00,  2.95it/s, loss=2.2, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 6:   0% 0/2 [00:00<?, ?it/s, loss=2.2, v_num=0]        #015Epoch 7:   0% 0/2 [00:00<?, ?it/s, loss=2.2, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 7:  50% 1/2 [00:00<00:00,  1.85it/s, loss=2.2, v_num=0]#015Epoch 7:  50% 1/2 [00:00<00:00,  1.85it/s, loss=2.2, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 7:  50% 1/2 [00:00<00:00,  1.85it/s, loss=2.14, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 93.36it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 7: 100% 2/2 [00:00<00:00,  3.17it/s, loss=2.14, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 7: 100% 2/2 [00:00<00:00,  2.94it/s, loss=2.14, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 7: 100% 2/2 [00:00<00:00,  2.93it/s, loss=2.14, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 7:   0% 0/2 [00:00<?, ?it/s, loss=2.14, v_num=0]        [1,mpirank:0,algo-1]<stdout>:#015Epoch 8:   0% 0/2 [00:00<?, ?it/s, loss=2.14, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 8:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.14, v_num=0]#015Epoch 8:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.14, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 8:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.11, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 89.88it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 8: 100% 2/2 [00:00<00:00,  3.18it/s, loss=2.11, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 8: 100% 2/2 [00:00<00:00,  2.95it/s, loss=2.11, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 8: 100% 2/2 [00:00<00:00,  2.94it/s, loss=2.11, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 8:   0% 0/2 [00:00<?, ?it/s, loss=2.11, v_num=0]        #015Epoch 9:   0% 0/2 [00:00<?, ?it/s, loss=2.11, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 9:  50% 1/2 [00:00<00:00,  1.81it/s, loss=2.11, v_num=0][1,mpirank:0,algo-1]<stdout>:#015Epoch 9:  50% 1/2 [00:00<00:00,  1.81it/s, loss=2.11, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 9:  50% 1/2 [00:00<00:00,  1.80it/s, loss=2.09, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 94.83it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 9: 100% 2/2 [00:00<00:00,  3.09it/s, loss=2.09, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 9: 100% 2/2 [00:00<00:00,  2.87it/s, loss=2.09, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 9: 100% 2/2 [00:00<00:00,  2.86it/s, loss=2.09, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 9:   0% 0/2 [00:00<?, ?it/s, loss=2.09, v_num=0]        #015Epoch 10:   0% 0/2 [00:00<?, ?it/s, loss=2.09, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 10:  50% 1/2 [00:00<00:00,  1.85it/s, loss=2.09, v_num=0]#015Epoch 10:  50% 1/2 [00:00<00:00,  1.85it/s, loss=2.09, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 10:  50% 1/2 [00:00<00:00,  1.85it/s, loss=2.06, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 95.91it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 10: 100% 2/2 [00:00<00:00,  3.16it/s, loss=2.06, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 10: 100% 2/2 [00:00<00:00,  2.95it/s, loss=2.06, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 10: 100% 2/2 [00:00<00:00,  2.94it/s, loss=2.06, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 10:   0% 0/2 [00:00<?, ?it/s, loss=2.06, v_num=0]        #015Epoch 11:   0% 0/2 [00:00<?, ?it/s, loss=2.06, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 11:  50% 1/2 [00:00<00:00,  1.89it/s, loss=2.06, v_num=0]#015Epoch 11:  50% 1/2 [00:00<00:00,  1.89it/s, loss=2.06, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 11:  50% 1/2 [00:00<00:00,  1.89it/s, loss=2.04, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 93.37it/s][1,mpirank:0,algo-1]<stdout>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 11: 100% 2/2 [00:00<00:00,  3.23it/s, loss=2.04, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 11: 100% 2/2 [00:00<00:00,  3.00it/s, loss=2.04, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 11: 100% 2/2 [00:00<00:00,  2.99it/s, loss=2.04, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 11:   0% 0/2 [00:00<?, ?it/s, loss=2.04, v_num=0]        #015Epoch 12:   0% 0/2 [00:00<?, ?it/s, loss=2.04, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 12:  50% 1/2 [00:00<00:00,  1.90it/s, loss=2.04, v_num=0]#015Epoch 12:  50% 1/2 [00:00<00:00,  1.90it/s, loss=2.04, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 12:  50% 1/2 [00:00<00:00,  1.89it/s, loss=2.03, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 97.54it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 12: 100% 2/2 [00:00<00:00,  3.21it/s, loss=2.03, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 12: 100% 2/2 [00:00<00:00,  2.99it/s, loss=2.03, v_num=0][1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 12: 100% 2/2 [00:00<00:00,  2.97it/s, loss=2.03, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 12:   0% 0/2 [00:00<?, ?it/s, loss=2.03, v_num=0]        #015Epoch 13:   0% 0/2 [00:00<?, ?it/s, loss=2.03, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 13:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.03, v_num=0]#015Epoch 13:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.03, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 13:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.02, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 94.56it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 13: 100% 2/2 [00:00<00:00,  3.17it/s, loss=2.02, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 13: 100% 2/2 [00:00<00:00,  2.95it/s, loss=2.02, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 13: 100% 2/2 [00:00<00:00,  2.94it/s, loss=2.02, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 13:   0% 0/2 [00:00<?, ?it/s, loss=2.02, v_num=0]        #015Epoch 14:   0% 0/2 [00:00<?, ?it/s, loss=2.02, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 14:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.02, v_num=0]#015Epoch 14:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.02, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 14:  50% 1/2 [00:00<00:00,  1.86it/s, loss=2.01, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 93.79it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 14: 100% 2/2 [00:00<00:00,  3.19it/s, loss=2.01, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 14: 100% 2/2 [00:00<00:00,  2.96it/s, loss=2.01, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 14: 100% 2/2 [00:00<00:00,  2.94it/s, loss=2.01, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 14:   0% 0/2 [00:00<?, ?it/s, loss=2.01, v_num=0]        #015Epoch 15:   0% 0/2 [00:00<?, ?it/s, loss=2.01, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 15:  50% 1/2 [00:00<00:00,  1.91it/s, loss=2.01, v_num=0]#015Epoch 15:  50% 1/2 [00:00<00:00,  1.91it/s, loss=2.01, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 15:  50% 1/2 [00:00<00:00,  1.91it/s, loss=1.99, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 92.94it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 15: 100% 2/2 [00:00<00:00,  3.26it/s, loss=1.99, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 15: 100% 2/2 [00:00<00:00,  3.04it/s, loss=1.99, v_num=0][1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 15: 100% 2/2 [00:00<00:00,  3.03it/s, loss=1.99, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 15:   0% 0/2 [00:00<?, ?it/s, loss=1.99, v_num=0]        #015Epoch 16:   0% 0/2 [00:00<?, ?it/s, loss=1.99, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 16:  50% 1/2 [00:00<00:00,  1.94it/s, loss=1.99, v_num=0][1,mpirank:0,algo-1]<stdout>:#015Epoch 16:  50% 1/2 [00:00<00:00,  1.94it/s, loss=1.99, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 16:  50% 1/2 [00:00<00:00,  1.94it/s, loss=1.97, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 94.73it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 16: 100% 2/2 [00:00<00:00,  3.29it/s, loss=1.97, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 16: 100% 2/2 [00:00<00:00,  3.06it/s, loss=1.97, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 16: 100% 2/2 [00:00<00:00,  3.05it/s, loss=1.97, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 16:   0% 0/2 [00:00<?, ?it/s, loss=1.97, v_num=0]        #015Epoch 17:   0% 0/2 [00:00<?, ?it/s, loss=1.97, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 17:  50% 1/2 [00:00<00:00,  1.90it/s, loss=1.97, v_num=0]#015Epoch 17:  50% 1/2 [00:00<00:00,  1.90it/s, loss=1.97, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 17:  50% 1/2 [00:00<00:00,  1.90it/s, loss=1.95, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 93.83it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 17: 100% 2/2 [00:00<00:00,  3.23it/s, loss=1.95, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 17: 100% 2/2 [00:00<00:00,  3.02it/s, loss=1.95, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 17: 100% 2/2 [00:00<00:00,  3.01it/s, loss=1.95, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 17:   0% 0/2 [00:00<?, ?it/s, loss=1.95, v_num=0]        #015Epoch 18:   0% 0/2 [00:00<?, ?it/s, loss=1.95, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 18:  50% 1/2 [00:00<00:00,  1.88it/s, loss=1.95, v_num=0]#015Epoch 18:  50% 1/2 [00:00<00:00,  1.88it/s, loss=1.95, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 18:  50% 1/2 [00:00<00:00,  1.87it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 92.73it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 18: 100% 2/2 [00:00<00:00,  3.21it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 18: 100% 2/2 [00:00<00:00,  2.97it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 18: 100% 2/2 [00:00<00:00,  2.96it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 18:   0% 0/2 [00:00<?, ?it/s, loss=1.92, v_num=0]        #015Epoch 19:   0% 0/2 [00:00<?, ?it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 19:  50% 1/2 [00:00<00:00,  1.88it/s, loss=1.92, v_num=0]#015Epoch 19:  50% 1/2 [00:00<00:00,  1.88it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 19:  50% 1/2 [00:00<00:00,  1.88it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation:   0% 0/1 [00:00<?, ?it/s]#033[A[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Validation DataLoader 0: 100% 1/1 [00:00<00:00, 95.10it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 19: 100% 2/2 [00:00<00:00,  3.21it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 19: 100% 2/2 [00:00<00:00,  2.99it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015                                                          #033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 19: 100% 2/2 [00:00<00:00,  2.98it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:#015Epoch 19: 100% 2/2 [00:01<00:00,  1.15it/s, loss=1.92, v_num=0]\u001b[0m\n",
      "\u001b[34m2022-08-16 06:48:29,671 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[32m2022-08-16 06:48:29,671 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[35m2022-08-16 06:48:29,657 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2022-08-16 06:48:29,657 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2022-08-16 06:48:29,658 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[36m2022-08-16 06:48:29,686 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\n",
      "2022-08-16 06:49:01 Uploading - Uploading generated training model\u001b[34m2022-08-16 06:48:59,699 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[34m2022-08-16 06:48:59,700 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32m2022-08-16 06:48:59,700 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[32m2022-08-16 06:48:59,701 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[36m2022-08-16 06:48:59,716 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[36m2022-08-16 06:48:59,716 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-08-16 06:49:27 Completed - Training job completed\n",
      "Training seconds: 1728\n",
      "Billable seconds: 1728\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\"batch_size\": 8}\n",
    "enable_local_mode_training = False\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = \"local_gpu\"\n",
    "    inputs = {\"data_path\": f\"file:///home/ec2-user/SageMaker/amazon-sagemaker-pytorch-lightning-distributed-training/data_semantics\"}\n",
    "else:\n",
    "    train_instance_type = \"ml.p3.16xlarge\"\n",
    "    inputs = {\"data_path\": data_url}\n",
    "    \n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"semantic_segmentation.py\",\n",
    "    \"source_dir\": \"code\",\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 4,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"pytorch-lightning\",\n",
    "    \"image_uri\": \"570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-custom:1.12.0-gpu-py38-cu113-ubuntu20.04-sagemaker-2.6.0-numproc\",\n",
    "    \"py_version\": \"py3\",\n",
    "    \"distribution\": {\"pytorchddp\":{\"enabled\": True}},\n",
    "    \"debugger_hook_config\": False,\n",
    "    \"disable_profiler\": True\n",
    "}\n",
    "\n",
    "estimator = PyTorch(**estimator_parameters)\n",
    "estimator.fit(inputs)#, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ccce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
